{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbhandler import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_matlab_file function Testing\n",
    "file_name = 'consensus1.mat'\n",
    "variable_of_interest = 'align3'\n",
    "h5py_object, data = load_matlab_file(file_name, variable_name=variable_of_interest)\n",
    "\n",
    "# DNA_seqs can be used to grab each raw_seqblock\n",
    "DNA_seqs = DNA_SeqBlocks(h5py_object=h5py_object, data=data) \n",
    "\n",
    "FILE_SIZE = DNA_seqs.size #Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in barcodes and truelens for the day\n",
    "df = pickle.load(open('pickle/v2BarcodesTruelen.pickle', 'rb'))\n",
    "BARCODES = df.barcode\n",
    "TRUELENS = df.true_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it works on a small dataset first.\n",
    "tinyDS = range(1, 10001)\n",
    "DS = range(1, FILE_SIZE+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET BFP-parsed seqblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bfp_list = []\n",
    "# rfp_list = []\n",
    "\n",
    "# for i in DS:\n",
    "#     end = TRUELENS[i]\n",
    "#     b, r = DNA_seqs.get_br(i, end)\n",
    "#     bfp_list.append(b)\n",
    "#     rfp_list.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pickle/raw_bfp.pickle', 'wb') as f:\n",
    "#     pickle.dump(bfp_list, f)\n",
    "\n",
    "# with open('pickle/raw_rfp.pickle', 'wb') as f:\n",
    "#     pickle.dump(rfp_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'em in \n",
    "bfp_list = pickle.load( open('pickle/raw_bfp.pickle', 'rb'))\n",
    "rfp_list = pickle.load( open('pickle/raw_rfp.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Correct and Incorrect SNPs in BFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 287 minutes\n",
    "# -----\n",
    "# For each qscore (except 32): track value[0] observed, value[1] incorrect\n",
    "each_qscore = {'correct' : 0, 'A' : 0, 'C' : 0, 'G' : 0, 'T' : 0, '-' : 0}\n",
    "\n",
    "# tracking = {'A' : { i : each_qscore for i in np.arange(32,127) },\n",
    "            # 'C' : { i : each_qscore for i in np.arange(32,127) },\n",
    "            # 'T' : { i : each_qscore for i in np.arange(32,127) },\n",
    "            # 'G' : { i : each_qscore for i in np.arange(32,127) },\n",
    "            # '-' : { i : each_qscore for i in np.arange(32,127) }}\n",
    "tracking = {'A' : { i : [0,0,0,0,0,0] for i in np.arange(32,127)},\n",
    "            'C' : {i : [0,0,0,0,0,0] for i in np.arange(32,127)},\n",
    "            'G' : {i : [0,0,0,0,0,0] for i in np.arange(32,127)},\n",
    "            'T' : {i : [0,0,0,0,0,0] for i in np.arange(32,127)},\n",
    "            '-' : {i : [0,0,0,0,0,0] for i in np.arange(32,127)}}\n",
    "\n",
    "def hot_encode(num):\n",
    "    if num == 65: return 1\n",
    "    elif num == 67: return 2\n",
    "    elif num == 71: return 3\n",
    "    elif num == 84: return 4\n",
    "    else: return 5\n",
    "    \n",
    "for bfp in bfp_list:\n",
    "    cov = int((len(bfp.columns)-4)/2) # New rename() will include it in the first two chars\n",
    "    for read_num in range(1, cov+1):\n",
    "        read = bfp[f'read{read_num}']\n",
    "        for nuc in range(len(read.index)):\n",
    "            true_nuc = bfp.iloc[nuc,0]\n",
    "            chr_tnuc = chr(true_nuc) # convert to a chr the targ\n",
    "            qscore = bfp[f'q{read_num}'].iloc[nuc] # The qscore\n",
    "            check = read.iloc[nuc] # \n",
    "            if check == true_nuc: # If correct. Add to observed\n",
    "                # tracking[chr_tnuc][qscore] = tracking[chr_tnuc].setdefault(qscore, {'correct' : 0,65:0,67:0,71:0,84:0,45:0})\n",
    "                # tracking[chr_tnuc][qscore]['correct'] += 1\n",
    "                tracking[chr_tnuc][qscore][0] += 1\n",
    "            else: # If they disagree\n",
    "                # tracking[chr_tnuc][qscore] = tracking[chr_tnuc].setdefault(qscore, {'correct':0, 65:0,67:0,71:0,84:0,45:0})\n",
    "                # tracking[chr_tnuc][qscore][chr(check)] += 1\n",
    "                tracking[chr_tnuc][qscore][hot_encode(check)] +=1        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include RFP with high coverage\n",
    "- Advised by Yiyang to increase sample size: principle is that high coverage contigs (4+) is true for all practical purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#78 minutes\n",
    "for rfp in rfp_list:\n",
    "    cov = int((len(rfp.columns)-4)/2)\n",
    "    if cov > 3:\n",
    "        for read_num in range(1, cov+1):\n",
    "            read = rfp[f'read{read_num}']\n",
    "            true = rfp.loc[:, 'contig']\n",
    "            for nuc in range(len(read.index)):\n",
    "                true_nuc = true.iloc[nuc]\n",
    "                chr_tnuc = chr(true_nuc) # convert to a chr the targ\n",
    "                qscore = rfp[f'q{read_num}'].iloc[nuc] # The qscore\n",
    "                check = read.iloc[nuc] # \n",
    "                if check == true_nuc: # If correct. Add to observed\n",
    "                    # tracking[chr_tnuc][qscore] = tracking[chr_tnuc].setdefault(qscore, {'correct' : 0,65:0,67:0,71:0,84:0,45:0})\n",
    "                    # tracking[chr_tnuc][qscore]['correct'] += 1\n",
    "                    tracking[chr_tnuc][qscore][0] += 1\n",
    "                else: # If they disagree\n",
    "                    # tracking[chr_tnuc][qscore] = tracking[chr_tnuc].setdefault(qscore, {'correct':0, 65:0,67:0,71:0,84:0,45:0})\n",
    "                    # tracking[chr_tnuc][qscore][chr(check)] += 1\n",
    "                    tracking[chr_tnuc][qscore][hot_encode(check)] +=1        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open( 'pickle/v2TrackingwRFP.pickle', 'wb') as f:\n",
    "#     pickle.dump(tracking, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking = pickle.load( open('pickle/v2TrackingwRFP.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Nucleotide-specific Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_vocab = ['correct', 'A', 'C', 'G', 'T', '-']\n",
    "skip = {'A' : 0, 'C' : 1, 'G': 2, 'T': 3}\n",
    "associate = {0 : 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "\n",
    "x = [i for i in np.arange(45, 126)]\n",
    "for nuc in ['A']:\n",
    "    ict = {'A' : [], 'C' : [], 'G' : [], 'T' : []} \n",
    "    for q, v in tracking[nuc].items():\n",
    "        if q < 45: continue\n",
    "        total = sum(v)\n",
    "        nuc_misreads = v[1:-1]\n",
    "        for idx, count in enumerate(nuc_misreads):\n",
    "            if idx == skip[nuc]:\n",
    "                continue\n",
    "            ict[associate[idx]].append(count)\n",
    "    for n in ['A', 'C', 'G', 'T']:\n",
    "        curr = ict[n]\n",
    "        if len(curr) < 2: continue\n",
    "        plt.plot(x, curr[:-1])\n",
    "        plt.legend(labels=[ 'C', 'G', 'T'])\n",
    "        plt.ylabel('Error Count')\n",
    "        plt.xlabel('Phred Quality Score')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in np.arange(39, 126)]\n",
    "for nuc in ['C']:\n",
    "    ict = {'A' : [], 'C' : [], 'G' : [], 'T' : []} \n",
    "    for q, v in tracking[nuc].items():\n",
    "        if q < 39: continue\n",
    "        total = sum(v)\n",
    "        nuc_misreads = v[1:-1]\n",
    "        for idx, count in enumerate(nuc_misreads):\n",
    "            if idx == skip[nuc]:\n",
    "                continue\n",
    "            ict[associate[idx]].append(count)\n",
    "    for n in ['A', 'C', 'G', 'T']:\n",
    "        curr = ict[n]\n",
    "        if len(curr) < 2: continue\n",
    "        plt.plot(x, curr[:-1])\n",
    "        plt.legend(labels=[ 'A', 'G', 'T'])\n",
    "        plt.ylabel('Error Count')\n",
    "        plt.xlabel('Phred Quality Score')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for nuc in ['G']:\n",
    "    ict = {'A' : [], 'C' : [], 'G' : [], 'T' : []} \n",
    "    for q, v in tracking[nuc].items():\n",
    "        if q < 39: continue\n",
    "        total = sum(v)\n",
    "        nuc_misreads = v[1:-1]\n",
    "        for idx, count in enumerate(nuc_misreads):\n",
    "            if idx == skip[nuc]:\n",
    "                continue\n",
    "            ict[associate[idx]].append(count)\n",
    "    for n in ['A', 'C', 'G', 'T']:\n",
    "        curr = ict[n]\n",
    "        if len(curr) < 2: continue\n",
    "        plt.plot(x, curr[:-1])\n",
    "        plt.legend(labels=[ 'A', 'C', 'T'])\n",
    "        plt.ylabel('Error Count')\n",
    "        plt.xlabel('Phred Quality Score')  \n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nuc in ['T']:\n",
    "    ict = {'A' : [], 'C' : [], 'G' : [], 'T' : []} \n",
    "    for q, v in tracking[nuc].items():\n",
    "        if q < 39: continue\n",
    "        total = sum(v)\n",
    "        nuc_misreads = v[1:-1]\n",
    "        for idx, count in enumerate(nuc_misreads):\n",
    "            if idx == skip[nuc]:\n",
    "                continue\n",
    "            ict[associate[idx]].append(count)\n",
    "    for n in ['A', 'C', 'G', 'T']:\n",
    "        curr = ict[n]\n",
    "        if len(curr) < 2: continue\n",
    "        plt.plot(x, curr[:-1])\n",
    "        plt.legend(labels=[ 'A', 'C', 'G'])\n",
    "        plt.ylabel('Error Count')\n",
    "        plt.xlabel('Phred Quality Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZE: Real Error of z and yyy\n",
    "\n",
    "#### Z Conflicts\n",
    "\n",
    "- Currently: Plot total wrong / all instances of reads vs true\n",
    "- Line of best fit plotted at Q > 45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start basic\n",
    "# Just 'A' and total wrong\n",
    "# Remember that 33 is useless. 32 does not exist.\n",
    "to_plot = {'A' : [], 'C' : [], 'G' : [], 'T' : []}\n",
    "\n",
    "\n",
    "raw_x = [i for i in np.arange(35, 127)] #34 is 0 in test\n",
    "\n",
    "for n in ['A', 'C', 'G', 'T']:\n",
    "    raw_y = []\n",
    "    focus = tracking[n]\n",
    "    for i in focus.keys():\n",
    "        if i < 35: continue\n",
    "        stats = focus.get(i)\n",
    "        total = sum(stats)\n",
    "        wrong = sum(stats[1:])\n",
    "        if total == 0:\n",
    "            raw_y.append(0)\n",
    "        else: raw_y.append(wrong/total)\n",
    "    to_plot[n] = raw_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the log if non-zero. Motive: linear graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for nuc in ['A', 'C', 'G', 'T']:\n",
    "    to_change = to_plot[nuc]\n",
    "    for i in range(len(to_change)):\n",
    "        if to_change[i] == 0: \n",
    "            to_change[i] = 0 \n",
    "        else: \n",
    "            to_change[i] = math.log(to_change[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing values with a mean (only present at low sample sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nuc in ['A', 'C', 'G', 'T']:\n",
    "    fix = to_plot[nuc]\n",
    "    for i in range(len(fix)):\n",
    "        if fix[i] == 0:\n",
    "            fix[i] = 0.5*(fix[i-1]+fix[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 4 nucleotides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, sharex=True, sharey=True, figsize=(10,10))\n",
    "plt.subplots_adjust(wspace=0,hspace=0)\n",
    "\n",
    "axes[0,0].scatter(raw_x, to_plot['A'], c='red',s=12)\n",
    "axes[0,1].scatter(raw_x, to_plot['C'], c='blue', s=12)\n",
    "axes[1,0].scatter(raw_x, to_plot['G'], c='green',s=12)\n",
    "axes[1,1].scatter(raw_x, to_plot['T'], c='purple',s=12)\n",
    "\n",
    "# Needs to be calculated below\n",
    "# axes[0,0].scatter(x, lobA, c='black')\n",
    "# axes[0,1].scatter(x, lobC, c='black')\n",
    "# axes[1,0].scatter(x, lobG, c='black')\n",
    "# axes[1,1].scatter(x, lobT, c='black')\n",
    "\n",
    "# # Check something...\n",
    "# for i in range(2):\n",
    "#     for j in range(2):\n",
    "#         axes[i,j].hlines(0.01, 35, 126)\n",
    "\n",
    "labels = ['A', 'C', 'G', 'T']\n",
    "# labels_with_lobf = ['A', '_lobfA', 'C', '_lobfC',  'G', '_lobfG',  'T', '1e-2']\n",
    "fig.legend(labels=labels, loc=\"right\")\n",
    "fig.suptitle('Associating Error Rate with Phred Q Score (n=455,157)', fontsize=25)\n",
    "fig.supxlabel('Phred Q scores', fontsize=20)\n",
    "fig.supylabel('Log_10 Adjusted Real Error Rate', fontsize=20)\n",
    "# fig.supylabel('Real Error Rate', fontsize=20)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100053"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count = 0\n",
    "# for rfp in rfp_list:\n",
    "#     if int((len(rfp.columns)-4)/2) > 3:\n",
    "#         count += 1\n",
    "# count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line of best fits\n",
    "- Calculated using ML and after Q = 45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# Assessing if contig is a viable thing to use for RFP Truth\n",
    "for x,rfp in enumerate(rfp_list):\n",
    "    if DNA_seqs.get_coverage_count(x+1) > 3:\n",
    "        if rfp.empty: continue\n",
    "        muts = rfp['changes']\n",
    "        if len(muts[muts==122]) > 1:\n",
    "            print(x+1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch to calculate line of best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired func y = mx + b => # params are m, b\n",
    "import torch\n",
    "p = torch.randn(2).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.arange(46, 127)).long()\n",
    "yA = torch.tensor(to_plot['A'][11:]).float()\n",
    "yC = torch.tensor(to_plot['C'][11:]).float()\n",
    "yG = torch.tensor(to_plot['G'][11:]).float()\n",
    "yT = torch.tensor(to_plot['T'][11:]).float()\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x,self.y = x,y\n",
    "        self.len = len(x)\n",
    "        \n",
    "    def __len__(self): return self.len\n",
    "    \n",
    "    def __getitem__(self, idx): return self.x[idx], self.y[idx]\n",
    "\n",
    "testdsA = torch.utils.data.DataLoader(Dataset(x,yA), batch_size=3)\n",
    "testdsC = torch.utils.data.DataLoader(Dataset(x,yC), batch_size=5)\n",
    "testdsG = torch.utils.data.DataLoader(Dataset(x,yG), batch_size=5)\n",
    "testdsT = torch.utils.data.DataLoader(Dataset(x,yT), batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(params, input):\n",
    "    a, b = params\n",
    "    return a*input + b\n",
    "\n",
    "import torch.functional as F\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "def loss_func(pred, targ):\n",
    "    # Let's use RMSE\n",
    "    return ((pred-targ)**2).sqrt()\n",
    "\n",
    "lr = 1e-5\n",
    "for i in range(1000):\n",
    "    loss_total = 0\n",
    "    n_obs = 0\n",
    "    for xin, yin in testdsT:\n",
    "        \n",
    "        pred = func(p, xin)\n",
    "        loss = loss_func(pred, yin).mean()\n",
    "        \n",
    "        loss.backward() \n",
    "        p.data -= p.grad.data * lr\n",
    "        p.grad = None\n",
    "        \n",
    "        n = len(xin)\n",
    "        loss_total += loss.item()*n\n",
    "        n_obs +=n\n",
    "        \n",
    "    print(loss_total/n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    a,b =p.data\n",
    "    return a*x+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobT = []\n",
    "for i in x:\n",
    "    lobT.append(f(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YYY Conflicts\n",
    "\n",
    "- Two main categories:\n",
    "    - Insertion (Target, non-agreeing reads have '-' for alignment)\n",
    "    - Deletion  (Only read has '-' for alignment) \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- Insertion types:\n",
    "    - Misaligned multi-insertions \n",
    "        - e.g. aligned as G-GG-AG / G**G**G*A*GAG when it should be GGG--AG / GGG**AG**AG) \n",
    "    - Multi-insertions (G--G / GAAG)\n",
    "        - [4y+]\n",
    "    - **Harmonious** Insert\n",
    "        - Matches left AND right\n",
    "    - **1x Discord** Insert\n",
    "        - Matches left OR right nuc\n",
    "    - **2x Discord** Insert\n",
    "        - No match with left OR right\n",
    "<br>      \n",
    "&nbsp;\n",
    "- Deletion types: (Note: the thing that *matches* is the truth that was deleted)\n",
    "    - **Harmonious** del\n",
    "        - Matches left AND right nuc\n",
    "    - **1X Discord** del \n",
    "        - Matches left OR right nuc \n",
    "    - **2x Discord** del \n",
    "        - No match with left OR right [e.g. A-G when others say A**C**G]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Difference in Error Counts between Nucleotides\n",
    "- Not shown: spikes at Q=126\n",
    "    - A ~ 20,000; C,G ~ 18,000; T ~ 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_vocab = ['correct', 'A', 'C', 'G', 'T', '-']\n",
    "tracking['A']\n",
    "\n",
    "for nuc in ['A', 'C', 'G', 'T']:\n",
    "    error_frequency = []\n",
    "    for q, v in tracking[nuc].items():\n",
    "        if q > 34:\n",
    "            error_frequency.append(sum(v[1:]))\n",
    "    q_related = [x for x in np.arange(35, 126)]\n",
    "    plt.plot(q_related, error_frequency[:-1])\n",
    "    # print(np.argmax(error_frequency[:-1]), max(error_frequency[:-1]))\n",
    "    # print(error_frequency[2])\n",
    "plt.legend(labels=['A', 'C', 'G', 'T'])\n",
    "plt.ylabel('Error Count')\n",
    "plt.xlabel('Phred Quality Score')\n",
    "    # max_freq = max(error_frequency[:-1])\n",
    "    # norm = [item/max_freq for item in error_frequency[:-1]]\n",
    "    # plt.plot(q_related, norm)\n",
    "    # plt.text(126, error_frequency[-1]-20_000, \"%d\" %error_frequency[-1] + nuc, ha=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**Resolution**\n",
    "- No Q score difference:\n",
    "    - (! is an insert) \\~\\~\\~ vs. ~!~ => \\~\\~\\~ wins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for where there are y conflicts (once the first y is found. Keep checking \"changes\" column along nuc++ until a non-y char is found)\n",
    "# Return the view of BFP or RFP with the continuous chunk of y's\n",
    "\n",
    "def is_insertion(df):\n",
    "    if (df['targ'][df['targ'] == 45]).empty: # If the target has NO indels\n",
    "        return True # Means it's a INSERTION\n",
    "    return False\n",
    "\n",
    "def is_harmonious(df, coverage):\n",
    "    for num in range(1, coverage+1):\n",
    "        read = df[f'read{num}']\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a LogReg model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ZConflicts(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        pass\n",
    "        \n",
    "class LinReg(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e45b5fe9540dcae422003c6a552f47067b28c6171ef36dc1ede64a9e99e7c78"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
