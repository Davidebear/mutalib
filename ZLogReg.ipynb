{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbhandler import *\n",
    "import pickle\n",
    "\n",
    "# load_matlab_file function Testing\n",
    "file_name = 'consensus1.mat'\n",
    "variable_of_interest = 'align3'\n",
    "h5py_object, data = load_matlab_file(file_name, variable_name=variable_of_interest)\n",
    "\n",
    "# DNA_seqs can be used to grab each raw_seqblock\n",
    "DNA_seqs = DNA_SeqBlocks(h5py_object=h5py_object, data=data) \n",
    "\n",
    "FILE_SIZE = DNA_seqs.size #Constant\n",
    "\n",
    "# Load 'em in \n",
    "# df = load(open('pickle/v2BarcodesTruelen.pickle', 'rb'))\n",
    "# BARCODES = df.barcode\n",
    "# TRUELENS = df.true_len\n",
    "\n",
    "bfp_list = pickle.load( open('pickle/v1rfp.pickle', 'rb'))\n",
    "rfp_list = pickle.load( open('pickle/v1bfp.pickle', 'rb'))\n",
    "\n",
    "# tracking = load( open('pickle/v2TrackingwRFP.pickle', 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastai.tabular import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastbook import one_hot, one_hot_decode\n",
    "toBeX = []\n",
    "toBeY = []\n",
    "conversion = dict(zip([65, 67, 71, 84, 45], [0, 1, 2, 3, 4]))\n",
    "# print(conversion\n",
    "\n",
    "def useableX(dfslice, idx):\n",
    "    temp_ct = dict(zip([65, 67, 71, 84, 45], [0, 0, 0, 0, 0]))\n",
    "    temp_q = dict(zip([65, 67, 71, 84, 45], [0, 0, 0, 0, 0]))\n",
    "    \n",
    "    cov = int( (len(dfslice.columns) - 4) / 2)\n",
    "    for read in range(cov): # Easy. Just adjust the count\n",
    "        nuc_read = dfslice[f'read{read+1}'].loc[idx]\n",
    "        temp_ct[nuc_read] += 1\n",
    "        nuc_qual = dfslice[f'q{read+1}'].loc[idx]\n",
    "        if nuc_qual > temp_q[nuc_read]: # Currently using keep the max. Is there some way to combine them?\n",
    "            # Note: my issue is that if I combine a 73 and a 105 for T. It's not better than a single A for 126 is it?\n",
    "            temp_q[nuc_read] = int(nuc_qual)\n",
    "    counts = torch.tensor(list(temp_ct.values()))\n",
    "    qscores = torch.tensor(list(temp_q.values()))\n",
    "    return torch.concat([counts, qscores])\n",
    "\n",
    "# Arbitrarily grabs slices (no regard for whether they have yyy or z conflicts)\n",
    "# for bfp in bfp_list[202:300]:\n",
    "\n",
    "#     print(bfp.index.name)\n",
    "#     for idx in range(len(bfp.index)):\n",
    "#         true_nuc = bfp['target'].iloc[idx] \n",
    "#         toBeY.append(conversion[true_nuc])\n",
    "#         toBeX.append(useableX(bfp, idx))\n",
    "# len(toBeX), len(toBeY)\n",
    "\n",
    "for bfp in bfp_list:\n",
    "    print(bfp.index.name)\n",
    "    if bfp.empty: continue\n",
    "    focus = bfp[bfp['changes'] ==122]\n",
    "    if focus.empty: continue\n",
    "    for idx in list(focus.index):\n",
    "        true_nuc = bfp['target'].loc[idx] \n",
    "        toBeY.append(conversion[true_nuc])\n",
    "        toBeX.append(useableX(bfp, idx))\n",
    "len(toBeX), len(toBeY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['A', 'C', 'G', 'T', '-']\n",
    "    \n",
    "raw_train_x = torch.stack(toBeX).float()\n",
    "raw_train_y = torch.stack(list(map(torch.tensor, toBeY))).long()\n",
    "\n",
    "train_x = raw_train_x/126\n",
    "train_y = raw_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rfpX(dfslice, idx, cov):\n",
    "    temp_ct = dict(zip([65, 67, 71, 84, 45], [0, 0, 0, 0, 0]))\n",
    "    temp_q  = dict(zip([65, 67, 71, 84, 45], [0, 0, 0, 0, 0]))\n",
    "\n",
    "    sample = [r for r in range(cov)] # Easy. Just adjust the count\n",
    "    to_use = (random.sample(sample, round(cov/2)))\n",
    "    print(to_use)\n",
    "    for read in to_use:\n",
    "        nuc_read = dfslice[f'read{read+1}'].loc[idx]\n",
    "        temp_ct[nuc_read] += 1\n",
    "        nuc_qual = dfslice[f'q{read+1}'].loc[idx]\n",
    "        if nuc_qual > temp_q[nuc_read]: # Currently using keep the max. Is there some way to combine them?\n",
    "            # Note: my issue is that if I combine a 73 and a 105 for T. It's not better than a single A for 126 is it?\n",
    "            temp_q[nuc_read] = int(nuc_qual)\n",
    "    counts = torch.tensor(list(temp_ct.values()))\n",
    "    qscores = torch.tensor(list(temp_q.values()))\n",
    "    return torch.concat([counts, qscores])\n",
    "\n",
    "\n",
    "for rfp in rfp_list:\n",
    "    if rfp.empty: continue\n",
    "    \n",
    "    cov = int( (len(rfp.columns)-4)/2)\n",
    "    if cov <= 3: continue \n",
    "    \n",
    "    focus = rfp[rfp['changes'] ==122]\n",
    "    if focus.empty: continue\n",
    "    \n",
    "    for idx in list(focus.index):\n",
    "        print(idx)\n",
    "        true_nuc = rfp['contig'].loc[idx] \n",
    "        toBeY.append(conversion[true_nuc])\n",
    "        toBeX.append(rfpX(rfp, idx, cov))\n",
    "len(toBeX), len(toBeY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70804, 10]) torch.Size([44816, 10])\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "raw_valid_x = torch.stack(toBeX).float()\n",
    "raw_valid_y = torch.stack(list(map(torch.tensor, toBeY))).long()\n",
    "\n",
    "valid_x = raw_valid_x/126\n",
    "valid_x = raw_valid_x/126\n",
    "valid_x = valid_x[-20_000:]\n",
    "valid_y = valid_y[-20_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([95620, 10]),\n",
       " torch.Size([95620]),\n",
       " torch.Size([20000, 10]),\n",
       " torch.Size([20000]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject to Change: I/O of LogReg Model\n",
    "- Input: 10 predictors (Act, Cct, Gct, Tct, Ict, errA, errC, errG, errT, errI)\n",
    "- Parameters: TBD -> use model selection techniques\n",
    "- Output: 5 classes (A, C, G, T, I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNPDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        super().__init__();\n",
    "        self.X, self.y = X,y\n",
    "        self.len = X.shape[0] \n",
    "    def __len__(self): return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SNPDataset(train_x, train_y)\n",
    "valid_ds = SNPDataset(valid_x, valid_y)\n",
    "\n",
    "batch_size=200\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogReg, self).__init__();\n",
    "        # self.input_dim, self.output_dim, self.hidden_dim = in_dim, out_dim, hidden_dim\n",
    "        # self.linear = nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=True)\n",
    "        \n",
    "        # Partial first layer\n",
    "        # self.linearA = nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "        # self.linearC = nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "        # self.linearG = nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "        # self.linearT = nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "        # self.linearI = nn.Linear(in_features=2, out_features=1, bias=True)   \n",
    "        self.linear1 = nn.Linear(in_features=10, out_features=20, bias=True)\n",
    "        \n",
    "        # Dense second layer (final)\n",
    "        self.linear2 = nn.Linear(in_features=20, out_features=5, bias=True)\n",
    "        \n",
    "        # the activation function used\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # outA = self.linearA(x[:,[0,5]]) #Act and AqualityTotal\n",
    "        # outC = self.linearC(x[:,[1,6]])\n",
    "        # outG = self.linearG(x[:,[2,7]])\n",
    "        # outT = self.linearT(x[:,[3,8]])\n",
    "        # outI = self.linearI(x[:,[4,9]])\n",
    "        \n",
    "        \n",
    "        # for out in [outA, outC, outG, outT, outI]:\n",
    "        #     # print(out)\n",
    "        #     out = self.relu(out)\n",
    "        # logits = torch.hstack([outA, outC, outG, outT, outI]).view(-1, 5) # Possible point of error. Confirm that .stack().view() are actually giving columns outA, outC, outG, outT, outI, etc.\n",
    "        # print(logits)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg()\n",
    "logreg = logreg.to('cuda')\n",
    "loss_func = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "optim = torch.optim.SGD(params=logreg.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  E01 | train loss: 142.1464 | train acc: 0.8580 | valid loss: 25.7437 | valid acc: 0.8887\n",
      "  E02 | train loss: 141.3671 | train acc: 0.8586 | valid loss: 25.5715 | valid acc: 0.8891\n",
      "  E03 | train loss: 140.5893 | train acc: 0.8593 | valid loss: 25.3950 | valid acc: 0.8893\n",
      "  E04 | train loss: 139.8630 | train acc: 0.8599 | valid loss: 25.2246 | valid acc: 0.8895\n",
      "  E05 | train loss: 139.5004 | train acc: 0.8606 | valid loss: 25.0554 | valid acc: 0.8897\n",
      "  E06 | train loss: 138.4573 | train acc: 0.8608 | valid loss: 24.8895 | valid acc: 0.8898\n",
      "  E07 | train loss: 137.6518 | train acc: 0.8613 | valid loss: 24.7270 | valid acc: 0.8901\n",
      "  E08 | train loss: 137.0449 | train acc: 0.8618 | valid loss: 24.5612 | valid acc: 0.8904\n",
      "  E09 | train loss: 135.9079 | train acc: 0.8622 | valid loss: 24.3991 | valid acc: 0.8909\n",
      "  E10 | train loss: 135.5674 | train acc: 0.8627 | valid loss: 24.2408 | valid acc: 0.8914\n",
      "  E11 | train loss: 135.0170 | train acc: 0.8631 | valid loss: 24.0806 | valid acc: 0.8918\n",
      "  E12 | train loss: 134.4141 | train acc: 0.8635 | valid loss: 23.9238 | valid acc: 0.8919\n",
      "  E13 | train loss: 133.7947 | train acc: 0.8638 | valid loss: 23.7700 | valid acc: 0.8919\n",
      "  E14 | train loss: 132.7430 | train acc: 0.8641 | valid loss: 23.6177 | valid acc: 0.8922\n",
      "  E15 | train loss: 132.2113 | train acc: 0.8645 | valid loss: 23.4648 | valid acc: 0.8924\n",
      "  E16 | train loss: 131.7921 | train acc: 0.8648 | valid loss: 23.3167 | valid acc: 0.8924\n",
      "  E17 | train loss: 130.9068 | train acc: 0.8653 | valid loss: 23.1696 | valid acc: 0.8929\n",
      "  E18 | train loss: 130.1653 | train acc: 0.8658 | valid loss: 23.0220 | valid acc: 0.8929\n",
      "  E19 | train loss: 129.3701 | train acc: 0.8663 | valid loss: 22.8793 | valid acc: 0.8931\n",
      "  E20 | train loss: 128.9467 | train acc: 0.8669 | valid loss: 22.7352 | valid acc: 0.8935\n",
      "  E21 | train loss: 128.4802 | train acc: 0.8672 | valid loss: 22.5930 | valid acc: 0.8936\n",
      "  E22 | train loss: 127.4519 | train acc: 0.8675 | valid loss: 22.4561 | valid acc: 0.8938\n",
      "  E23 | train loss: 127.2216 | train acc: 0.8679 | valid loss: 22.3194 | valid acc: 0.8942\n",
      "  E24 | train loss: 126.1292 | train acc: 0.8684 | valid loss: 22.1800 | valid acc: 0.8943\n",
      "  E25 | train loss: 125.9954 | train acc: 0.8688 | valid loss: 22.0470 | valid acc: 0.8945\n",
      "  E26 | train loss: 125.0532 | train acc: 0.8692 | valid loss: 21.9121 | valid acc: 0.8946\n",
      "  E27 | train loss: 124.5802 | train acc: 0.8694 | valid loss: 21.7823 | valid acc: 0.8949\n",
      "  E28 | train loss: 123.9985 | train acc: 0.8697 | valid loss: 21.6520 | valid acc: 0.8951\n",
      "  E29 | train loss: 123.3139 | train acc: 0.8700 | valid loss: 21.5218 | valid acc: 0.8954\n",
      "  E30 | train loss: 123.3927 | train acc: 0.8701 | valid loss: 21.3983 | valid acc: 0.8954\n",
      "  E31 | train loss: 122.0759 | train acc: 0.8705 | valid loss: 21.2727 | valid acc: 0.8954\n",
      "  E32 | train loss: 121.9573 | train acc: 0.8708 | valid loss: 21.1478 | valid acc: 0.8955\n",
      "  E33 | train loss: 121.0983 | train acc: 0.8712 | valid loss: 21.0242 | valid acc: 0.8957\n",
      "  E34 | train loss: 120.6343 | train acc: 0.8715 | valid loss: 20.9047 | valid acc: 0.8959\n",
      "  E35 | train loss: 120.1932 | train acc: 0.8718 | valid loss: 20.7884 | valid acc: 0.8961\n",
      "\n",
      "Model training complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weights_prior = logreg.linear.weight.detach().numpy().copy()\n",
    "\n",
    "logreg.train()\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # Observations\n",
    "    n_obs = 0\n",
    "    loss_sum = 0\n",
    "    n_right = 0\n",
    "    \n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to('cuda')\n",
    "        yb = yb.to('cuda')\n",
    "        optim.zero_grad() # Reset gradient\n",
    "        \n",
    "        # Forward pass.\n",
    "        logit = logreg.forward(xb) # Get logit_batch\n",
    "        loss = loss_func(logit, yb) # Calculate loss.\n",
    "        \n",
    "        # Update. Learn.\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # Update observation metrics\n",
    "        n_obs += len(yb)\n",
    "        loss_sum += loss.item() * n_obs\n",
    "        n_right += (logit.argmax(1) == yb.long()).float().sum().item()\n",
    "    \n",
    "    loss_train = loss_sum / n_obs\n",
    "    acc_train = n_right / n_obs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Observations\n",
    "        n_obs = 0\n",
    "        loss_sum = 0\n",
    "        n_right = 0\n",
    "        \n",
    "        for xb, yb in valid_dl:\n",
    "            xb = xb.to('cuda')\n",
    "            yb = yb.to('cuda')\n",
    "            # Forward pass.\n",
    "            logit = logreg.forward(xb) # Get logit_batch\n",
    "            loss = loss_func(logit, yb) # Calculate loss.\n",
    "            \n",
    "            # Update observation metrics\n",
    "            n_obs += len(yb)\n",
    "            loss_sum += loss.item() * n_obs\n",
    "            n_right += (logit.argmax(1) == yb.long()).float().sum().item()\n",
    "        \n",
    "    loss_valid = loss_sum / n_obs\n",
    "    acc_valid = n_right / n_obs\n",
    "    \n",
    "    disp_prog_freq = 1\n",
    "    if ((epoch % disp_prog_freq == 0) | (epoch == 0) | (epoch == n_epochs)): # Show for first epoch, last epoch, and based on desired frequency\n",
    "        print(f'  E{(epoch):02d} | train loss: {loss_train:.4f} | train acc: {acc_train:.4f} | valid loss: {loss_valid:.4f} | valid acc: {acc_valid:.4f}')\n",
    "        \n",
    "print('\\nModel training complete.\\n')\n",
    "\n",
    "# weights_after = logreg.linear.weight.detach().numpy().copy()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  valid loss: 33.2568 | valid acc: 0.8718\n",
      "\n",
      "Model validation complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "     # Observations\n",
    "    n_obs = 0\n",
    "    loss_sum = 0\n",
    "    n_right = 0\n",
    "    \n",
    "    for xb, yb in valid_dl:\n",
    "        xb = xb.to('cuda')\n",
    "        yb = yb.to('cuda')\n",
    "        # Forward pass.\n",
    "        logit = logreg.forward(xb) # Get logit_batch\n",
    "        loss = loss_func(logit, yb) # Calculate loss.\n",
    "        \n",
    "        # Update observation metrics\n",
    "        n_obs += len(yb)\n",
    "        loss_sum += loss.item() * n_obs\n",
    "        n_right += (logit.argmax(1) == yb.long()).float().sum().item()\n",
    "    \n",
    "    loss_train = loss_sum / n_obs\n",
    "    acc_train = n_right / n_obs\n",
    "    \n",
    "    disp_prog_freq = 1\n",
    "    if ((epoch % disp_prog_freq == 0) | (epoch == 0) | (epoch == n_epochs)): # Show for first epoch, last epoch, and based on desired frequency\n",
    "        print(f'  valid loss: {loss_train:.4f} | valid acc: {acc_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
