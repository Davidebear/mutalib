{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed libraries. Load needed objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad hoc library\n",
    "from sbhandler import * \n",
    "\n",
    "# Data Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Store and Load Objects\n",
    "from pickle import load, dump  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembly Matrices (pd.DataFrame objects)\n",
    "RFP_LIST = load( open('pickle/v1rfp.pickle', 'rb'))\n",
    "BFP_LIST = load( open('pickle/v1bfp.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features to generate for Extended Logistic Regression Model\n",
    "- Type (2):      &emsp;     &emsp;       &emsp; &emsp; &emsp; &emsp;  isSNP, isInsertion, (isDeletion ~ implied by other two)\n",
    "- Length (1):    &emsp;  &emsp;  &emsp; &emsp;      &emsp;   &nbsp;        isSingle, (isMulti)\n",
    "- Near another Indel (1):   &emsp;  isNear, (isFar)\n",
    "- Neighbors (1):        &emsp;   &emsp; &ensp; &nbsp;   &emsp;   isConcord, (isDiscord)\n",
    "- Observed Error Rate [5]:  &nbsp;    Map Phred Q Score to SNP/Indel (1-epsilon) for the Nucleotide *-OR-* (epsilon * conditional error)\n",
    "    - Multiply instances\n",
    "- Count [5]: &emsp; &emsp; &emsp; &emsp; &emsp; &ensp; Explicit read count of each nuc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Column: Variation Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cov(df): return int((len(df.columns)-4-4)/2)\n",
    "\n",
    "def pd_reads_truncater(df, list_reads):\n",
    "    # Removes meaningless indels that only arise from start, end alignment\n",
    "    ret_list = []\n",
    "\n",
    "    for read in list_reads:\n",
    "        nan_indel_mask = read[read != 45]\n",
    "        first_valid = nan_indel_mask.first_valid_index()\n",
    "        last_valid = nan_indel_mask.last_valid_index()\n",
    "        ret_list.append( (first_valid, last_valid) )\n",
    "        # Replace all 45's before and after the idx tuple with 1\n",
    "        if first_valid is not None and first_valid!= df.index[0]:\n",
    "            read.loc[:first_valid] = 1 # start and excluding\n",
    "        if last_valid is not None and last_valid != df.index[-1]:\n",
    "            read.loc[last_valid+1:] = 1 # past and end\n",
    "            \n",
    "    return ret_list\n",
    "\n",
    "def add_variation_type(df, cov):\n",
    "    \"\"\"Adds Variation Type: Assumes `is_indel` column exists\n",
    "\n",
    "    Filters (conditional) by at least a single 45 instance in a row.\n",
    "    \n",
    "    If: ground truth is a 45 (-) -> It's an insertion.\n",
    "    Else: It's a deletion.\n",
    "    Args:\n",
    "        df (DataFrame): RFP or BFP\n",
    "        cov (int): coverage; no. of reads\n",
    "        Return: adds `is_snip` and `is_ins` boolean column\n",
    "    \"\"\"\n",
    "    df['is_ins'] = np.where( df[['target', 'is_indel']].eq([45, True], axis=1).all(axis=1), True, False)\n",
    "\n",
    "    def check_row_variation(df, cov):\n",
    "        if cov == 1:\n",
    "            reads = df.iloc[:, 1]\n",
    "            targ = df.iloc[:, 0]\n",
    "            return (reads != targ)\n",
    "        else:\n",
    "            reads = df.iloc[:, 1:cov+1]\n",
    "            targ = df.iloc[:, 0]\n",
    "            return ( reads.sum(axis=1) != (targ*cov))\n",
    "    df['is_snp'] = np.where( df['is_indel'] == False, check_row_variation(df, cov), False)\n",
    "    \n",
    "    # Double conditional\n",
    "    # First... check if its an indel -> immediately goes to False\n",
    "    # Then... check if the target and at least one read disagree ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Length Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSingleSNP(df, idx):\n",
    "    pass\n",
    "\n",
    "def isSingleDel(df, cov, idx, adjust, rfp=False): # Checks if read has only one indel.\n",
    "    # Assumes it's a del row.\n",
    "    if rfp: # Only difference is use of .loc instead of .iloc\n",
    "        def check_del_left(df, idx, cov):\n",
    "            if idx == df.index[0]: return False\n",
    "            return (df.iloc[idx-adjust-1, 1:cov+1] == 45).any()\n",
    "        def check_del_right(df, idx, cov):\n",
    "            if idx == df.index[-1]: return False\n",
    "            return ((df.iloc[idx-adjust+1, 1:cov+1] == 45).any())\n",
    "        # print(idx, df.index.name)\n",
    "        # print(df.iloc[idx-adjust, -1])\n",
    "        # print(check_del_right(df, idx, cov))\n",
    "        # print(check_del_left(df, idx, cov))\n",
    "        df.iloc[idx-adjust, -1] = False if ( check_del_right(df, idx, cov) and check_del_left(df, idx, cov)) else True\n",
    "        # print(idx, df.index.name)\n",
    "    else:\n",
    "        def check_del_left(df, idx, cov):\n",
    "            if idx == 0: return False\n",
    "            return (df.iloc[idx-1, 1:cov+1] == 45).any()\n",
    "        def check_del_right(df, idx, cov):\n",
    "            if idx == len(df) - 1: return False\n",
    "            return ((df.iloc[idx+1, 1:cov+1] == 45).any())\n",
    "        df.iloc[idx, -1] = False if ( check_del_right(df, idx, cov) and check_del_left(df, idx, cov)) else True\n",
    "    # isMulti if a single 45 is present left or right.\n",
    "    \n",
    "def isSingleIns(df, idx, adjust, rfp=False): # Checks if target has only one indel.\n",
    "    if rfp:\n",
    "        def check_right(df, idx):\n",
    "            if idx == df.index[-1]: return False\n",
    "            return (df.iloc[idx-adjust+1, 0] == 45)\n",
    "        def check_left(df, idx):\n",
    "            if idx == df.index[0]: return False\n",
    "            return df.iloc[idx-adjust-1, 0] == 45\n",
    "        # print(idx, df.index.name)\n",
    "        df.iloc[idx-adjust, -1] = False if (check_right(df, idx) and check_left(df, idx)) else True\n",
    "        # print(idx, df.index.name)\n",
    "    else:\n",
    "        def check_right(df, idx):\n",
    "            if idx == len(df) - 1: return False\n",
    "            return (df.iloc[idx+1, 0] == 45)\n",
    "        def check_left(df, idx):\n",
    "            if idx == 0: return False\n",
    "            return (df.iloc[idx-1, 0] == 45)\n",
    "        # print(idx)\n",
    "        df.iloc[idx, -1] = False if (check_right(df, idx) and check_left(df, idx)) else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variation_length(df, cov, rfp=False):\n",
    "    df['is_single'] = [True for _ in range(len(df))]\n",
    "    adjust = df.index[0]\n",
    "\n",
    "    dels = df.iloc[np.where(( df[['is_indel', 'is_ins']] == [True, False] ).all(axis=1) == True)[0], :]\n",
    "    to_check = dels.index.to_list()\n",
    "    if not to_check == []:\n",
    "        for i in to_check:\n",
    "            isSingleDel( df, cov, i, adjust, rfp=rfp)\n",
    "\n",
    "    ins = df.iloc[np.where((df[['is_indel', 'is_ins']] == [True, True]).all(axis=1) == True)[0], :]\n",
    "    to_check = ins.index.to_list()\n",
    "    if not to_check:\n",
    "        for i in to_check:\n",
    "            isSingleIns(df, i, adjust, rfp=rfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Engineered Features to RFP and BFP pd.DataFrame objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, cov, indel_parse=False, rfp=False):\n",
    "    if indel_parse:\n",
    "        nan_mask = df.iloc[:, 0:cov+1][df!=45]\n",
    "        df['is_indel'] = np.where(nan_mask.count(axis = 1) == cov+1, False, True) #cov + 1 because targ/contig included\n",
    "        \n",
    "    # isSNP, isIns, isDel\n",
    "    add_variation_type(df, cov)\n",
    "    \n",
    "    # isSingle, isMulti\n",
    "    add_variation_length(df, cov, rfp=rfp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_progress(n, each=False):\n",
    "    if each:\n",
    "        print(n, end=\"\\r\")\n",
    "    else:\n",
    "        if (n % 3551 == 0): print(f'{n / 355104:.0%}', end='\\r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355103\r"
     ]
    }
   ],
   "source": [
    "for n, bfp in enumerate(BFP_LIST):\n",
    "    if bfp.empty: continue\n",
    "    get_progress(n, each=True)\n",
    "    add_features(bfp, get_cov(bfp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle/v2bfp.pickle', 'wb') as f:\n",
    "    dump(BFP_LIST, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355103\r"
     ]
    }
   ],
   "source": [
    "for n, rfp in enumerate(RFP_LIST):\n",
    "    if rfp.empty: continue\n",
    "    get_progress(n, each=True)\n",
    "    \n",
    "    add_features(rfp, get_cov(rfp), rfp=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle/v2rfp.pickle', 'wb') as f:\n",
    "    dump(RFP_LIST, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, bfp in enumerate(umm):\n",
    "    if bfp.empty: continue\n",
    "    a = (bfp[bfp['is_snp'] == True])\n",
    "    b = (bfp[bfp['mutations'] == 120])\n",
    "    if not b.empty:\n",
    "        if not a.empty:\n",
    "            print(bfp.index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           target  read1  q1  changes  contig  mutations  is_indel\n",
      "01B--1046                                                         \n",
      "259            84     65  51       32      65        120     False\n",
      "702            45     67  91       32      67        120      True\n"
     ]
    }
   ],
   "source": [
    "# for bfp in BFP_LIST[1000:]:\n",
    "#     if bfp.empty: continue\n",
    "#     find = bfp[bfp['mutations'] != 32]\n",
    "#     if find.empty: continue\n",
    "#     print(find)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to facilitate feature-type filtering -> will be used to grab specific rows for training\n",
    "def pd_filter(df, columns, conditions, all=True):\n",
    "    columns_to_filter_by = df[columns]\n",
    "    if all:\n",
    "        return df.iloc[np.where( (columns_to_filter_by == conditions).all(axis=1))[0]]\n",
    "    else:\n",
    "        return df.iloc[np.where( (columns_to_filter_by == conditions).any(axis=1))[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_ASSEMBLY_MATRICES = 355104\n",
    "TOTAL_SAMPLES = TOTAL_ASSEMBLY_MATRICES * 50\n",
    "\n",
    "PERCENT_RED = 0.4\n",
    "PERCENT_BLUE = 0.6\n",
    "\n",
    "PERCENT_SNP = 0.3\n",
    "PERCENT_INS = 0.4\n",
    "PERCENT_DEL = 0.3\n",
    "PERCENT_REGULAR = 0.1\n",
    "\n",
    "# To change as more features are added\n",
    "SAMPLE_SHAPE = (14)\n",
    "LABEL_SHAPE = (5)\n",
    "\n",
    "COUNT_MAPPING = {65 : 0, 67 : 0, 71 : 2, 84: 3, 45 : 4,}\n",
    "\n",
    "PROB_MAPPING = dict(zip([65, 67, 71, 84, 45], [5, 6, 7, 8, 9]))\n",
    "\n",
    "CATEGORY_MAPPING = dict(zip(['is_snp', 'is_ins', 'is_single', 'is_rfp'], [0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by Quality Score... then query by ground truth and read.\n",
    "CEM_INDELS = load( open('pickle/cem_indels_tensor.pickle', 'rb'))\n",
    "CEM_SNIPS = load( open('pickle/cem_snips_tensor.pickle', 'rb'))\n",
    "\n",
    "QADJ_INDELS = np.zeros( (5, 94) )\n",
    "QADJ_SNIPS = np.zeros( (4, 94) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_indels = load( open('pickle/v1TrackingIndelswRFP.pickle', 'rb'))\n",
    "tracking_snips = load( open('pickle/v1TrackingSnipswRFP.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuc_hot_encode(nuc):\n",
    "    dict = {'A' : 0, 'C': 1, 'G' : 2, 'T' : 3, '-' : 4}\n",
    "    return dict.get(nuc)\n",
    "\n",
    "def to_epsilon(tracking, nuc, storage):\n",
    "    focus = tracking[nuc]\n",
    "    to_add = []\n",
    "    for q, i in focus.items():\n",
    "        print(i)\n",
    "        if np.sum(i == 0): to_add.append(None)\n",
    "        else:\n",
    "            print(np.sum(i[1:]))\n",
    "            to_add.append(np.sum(i[1:])/np.sum(i))\n",
    "    storage[nuc_hot_encode(nuc)] = to_add\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nuc in ['A', 'C', 'G','T', '-']:\n",
    "    to_epsilon(tracking_indels, nuc, QADJ_INDELS)\n",
    "for nuc in ['A', 'C', 'G', 'T']:\n",
    "    to_epsilon(tracking_snips, nuc, QADJ_SNIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHRED to ERROR RATE\n",
    "def convert_to_epsilon(nuc, zero_shifted_quality, indel=True):\n",
    "    database = QADJ_INDELS if indel else QADJ_SNIPS\n",
    "    return database[nuc][zero_shifted_quality]\n",
    "    \n",
    "# PHRED to CONDITIONAL SEQ ERROR\n",
    "def convert_to_cond_seq_error(nuc, ground_truth, zero_shifted_quality, indel=True):\n",
    "    database = CEM_INDELS if indel else CEM_SNIPS\n",
    "    cond_seq_error = database[zero_shifted_quality][PROB_MAPPING[nuc], PROB_MAPPING[ground_truth]]\n",
    "    return cond_seq_error * convert_to_epsilon(nuc, zero_shifted_quality, indel=indel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_df_run(df, all_samples, all_labels, reads_max, rfp=False, enforce_cov = False, cov_min=4, cov_max=20):\n",
    "    is_rfp = rfp\n",
    "    cov = get_cov(df)\n",
    "    if enforce_cov and (cov < cov_min or cov > cov_max): return\n",
    "\n",
    "    reads_idxs = [i for i in range(1, cov+1)]\n",
    "    \n",
    "    if cov > reads_max: \n",
    "        reads_to_use = np.random.sample(reads_idxs)[0:reads_max]\n",
    "    else:\n",
    "        reads_to_use = reads_idxs\n",
    "        \n",
    "    # Get SNPs only\n",
    "    snps = df.iloc[np.where( (df['is_snp'] == True))[0]]\n",
    "    \n",
    "    # Get indels only\n",
    "    ins = df.iloc[np.where( (df['is_ins'] == True))[0]]\n",
    "    dels = pd_filter(df, ['is_indel', 'is_ins'], [True, False])\n",
    "    \n",
    "    # Get regulars?\n",
    "    regs = pd_filter(df, ['is_indel', 'is_snp'], [False, False])\n",
    "    \n",
    "    def extend_data(s, l):\n",
    "        all_samples.extend(s)\n",
    "        all_labels.extend(l)\n",
    "    \n",
    "    # Actually update it\n",
    "    # if count_snp < max_snp:\n",
    "    if not snps.empty:\n",
    "        s, l = sample_label_generator(df, snps, cov, reads_to_use, all_samples, all_labels, is_snps=True, rfp=is_rfp)\n",
    "        count_snp += len(l)\n",
    "        extend_data(s, l)\n",
    "    \n",
    "    # if count_ins < max_ins:\n",
    "    if not ins.empty:\n",
    "        s, l = sample_label_generator(df, ins, cov, reads_to_use, all_samples, all_labels, is_ins=True, rfp=is_rfp)\n",
    "        count_ins += len(l)\n",
    "        extend_data(s, l)\n",
    "\n",
    "    # if count_del < max_del:\n",
    "    if not dels.empty:\n",
    "        s, l = sample_label_generator(df, dels, cov, reads_to_use, all_samples, all_labels,rfp=rfp)\n",
    "        count_del += len(l)\n",
    "        extend_data(s, l)\n",
    "    \n",
    "    # if count_reg < max_reg:\n",
    "    if not regs.empty:\n",
    "        s, l = sample_label_generator(df, regs, cov, reads_to_use, all_samples, all_labels, is_snps=True)\n",
    "        count_reg += len(l)\n",
    "        extend_data(s, l)\n",
    "\n",
    "def sample_label_generator(df, filtered_data, cov, reads_to_use, rfp=False, is_snps=False, is_ins=False):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    \n",
    "    idx_iter = len(filtered_data)\n",
    "    \n",
    "    for i in filtered_data.index.to_list():\n",
    "        sample = torch.empty(SAMPLE_SHAPE)\n",
    "        label = torch.zeros(LABEL_SHAPE)\n",
    "        count = np.zeros(5)\n",
    "        prob = np.ones(5)\n",
    "        \n",
    "        if is_snps:\n",
    "            category = np.array([1, 0, 0, 1]) if rfp else np.array([1, 0, 0, 0])\n",
    "        elif is_ins:\n",
    "            category = np.array([0, 1, 0, 1]) if rfp else np.array([0, 1, 0, 0])\n",
    "        else:\n",
    "            category = np.array([0, 0, 0, 1]) if rfp else np.array([0, 0, 0, 0])\n",
    "        \n",
    "        # Row by row...\n",
    "        ground = df['target'].loc[i]\n",
    "        truth = df['contig'].loc[i] if rfp else ground\n",
    "            \n",
    "        for r in reads_to_use:\n",
    "            nuc = df[f'read{r}'].loc[i]\n",
    "            quality = df[f'q{r}'].loc[i]\n",
    "            if nuc == ground:\n",
    "                prob[PROB_MAPPING.get(nuc)] *= (1 - convert_to_epsilon('snp', nuc, quality) )\n",
    "            else: \n",
    "                prob[PROB_MAPPING.get(nuc)] *= convert_to_epsilon('snp', nuc, quality) * convert_to_cond_seq_error('snp', nuc, ground, quality)\n",
    "            count[PROB_MAPPING.get(nuc)] += 1\n",
    "            \n",
    "        if df['is_single'].loc[i] is True:\n",
    "            category[CATEGORY_MAPPING.get('is_single')] = 1\n",
    "            \n",
    "        # Update sample, label\n",
    "        sample[0:5] = count\n",
    "        sample[5:10] = prob\n",
    "        sample[10:14] = category\n",
    "        \n",
    "        label[PROB_MAPPING.get(truth)] = 1\n",
    "        \n",
    "        samples.append(sample)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return samples, labels\n",
    "\n",
    "def get_data(df_list):\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "    \n",
    "    percent_of_total = PERCENT_BLUE\n",
    "    if rfp: percent_of_total = PERCENT_RED\n",
    "    \n",
    "    size = percent_of_total * total\n",
    "    one_percent = 0.01 * size\n",
    "    \n",
    "    if random_select:\n",
    "        random_idxs = np.random.permutation(TOTAL_ASSEMBLY_MATRICES)\n",
    "        \n",
    "        for n, idx in enumerate(random_idxs):\n",
    "            get_progress(n, each=True)\n",
    "            df = df_list[idx]\n",
    "            if df.empty: continue\n",
    "            \n",
    "            single_df_run(df, all_samples, all_labels, reads_max)\n",
    "    else:\n",
    "        for n, df in enumerate(df_list):\n",
    "            get_progress(n, each=True)\n",
    "            if df.empty: continue\n",
    "            \n",
    "            single_df_run(df, all_samples, all_labels, reads_max)\n",
    "            \n",
    "    return all_samples, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFDataSet():\n",
    "    def __init__(self, df_list, cond_ind, cond_snps, qadj_ind, qadj_snps, is_rfp=False, enforce_cov=False, cov_min=4, cov_max=20, reads_max=3):\n",
    "        self.dfs = df_list\n",
    "        self.total_assembly_matrices = 355104\n",
    "        \n",
    "        self.cond_ind, self.cond_snps = cond_ind, cond_snps\n",
    "        self.qadj_ind, self.qadj_snps = qadj_ind, qadj_snps\n",
    "        \n",
    "        self.all_samples, self.all_labels = [], []\n",
    "        \n",
    "        self.is_rfp = is_rfp\n",
    "        self.enforce_cov = enforce_cov\n",
    "        self.cov_min = cov_min\n",
    "        self.cov_max = cov_max\n",
    "        self.reads_max = reads_max\n",
    "        \n",
    "        self.composition = False\n",
    "        self.nuc_to_idx = dict(zip(['A', 'C', 'G', 'T', '-', 65, 67, 71, 84, 45], [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]))\n",
    "    \n",
    "    def set_composition(self, percent_snps, percent_dels, percent_ins, percent_regs, total_samples):\n",
    "        \n",
    "        self.max_snps = percent_snps * total_samples\n",
    "        self.max_ins = percent_ins * total_samples\n",
    "        self.max_dels = percent_dels * total_samples\n",
    "        self.max_regs = percent_regs * total_samples\n",
    "        \n",
    "        self.count_snps, self.count_ins, self.count_dels, self.count_regs = 0,0,0,0\n",
    "        \n",
    "        self.composition = True\n",
    "    \n",
    "    def to_epsilon(self, nuc, zeroed_quality, is_snp=False):\n",
    "        if is_snp: return self.qadj_snps[self.nuc_to_idx.get(nuc)][zeroed_quality]\n",
    "        return self.qadj_ind[self.nuc_to_idx.get(nuc)][zeroed_quality]\n",
    "    \n",
    "    def to_cond_error(self, nuc, zeroed_quality, ground_truth, is_snp=False):\n",
    "        if is_snp:\n",
    "            error = to_epsilon(self, nuc, zeroed_quality, is_snp=True)\n",
    "            return error * self.cond_snps[zeroed_quality][self.nuc_to_idx.get(nuc), self.nuc_to_idx.get(ground_truth)]\n",
    "        error = to_epsilon(self, nuc, zeroed_quality, is_snp=False)\n",
    "        return error * self.cond_ind[zeroed_quality][self.nuc_to_idx.get(nuc), self.nuc_to_idx.get(ground_truth)]\n",
    "    \n",
    "    def pd_filter(df, columns, conditions, all=True):\n",
    "        columns_to_filter_by = df[columns]\n",
    "        if all:\n",
    "            return df.iloc[np.where( (columns_to_filter_by == conditions).all(axis=1))[0]]\n",
    "        else:\n",
    "            return df.iloc[np.where( (columns_to_filter_by == conditions).any(axis=1))[0]]\n",
    "    \n",
    "    def sample_label_generator(self, df, filtered_data, cov, reads_to_use, is_snps=False, is_ins=False):\n",
    "        samples = []\n",
    "        labels = []\n",
    "        \n",
    "        idx_iter = len(filtered_data)\n",
    "        \n",
    "        for i in filtered_data.index.to_list():\n",
    "            sample = torch.empty(SAMPLE_SHAPE)\n",
    "            label = torch.zeros(LABEL_SHAPE)\n",
    "            count = np.zeros(5)\n",
    "            prob = np.ones(5)\n",
    "            \n",
    "            if is_snps:\n",
    "                category = np.array([1, 0, 0, 1]) if self.is_rfp else np.array([1, 0, 0, 0])\n",
    "            elif is_ins:\n",
    "                category = np.array([0, 1, 0, 1]) if self.is_rfp else np.array([0, 1, 0, 0])\n",
    "            else:\n",
    "                category = np.array([0, 0, 0, 1]) if self.is_rfp else np.array([0, 0, 0, 0])\n",
    "            \n",
    "            # Row by row...\n",
    "            ground = df['target'].loc[i]\n",
    "            truth = df['contig'].loc[i] if self.is_rfp else ground\n",
    "                \n",
    "            for r in reads_to_use:\n",
    "                nuc = df[f'read{r}'].loc[i]\n",
    "                quality = df[f'q{r}'].loc[i]\n",
    "                if nuc == ground:\n",
    "                    prob[PROB_MAPPING.get(nuc)] *= (1 - convert_to_epsilon('snp', nuc, quality) )\n",
    "                else: \n",
    "                    prob[PROB_MAPPING.get(nuc)] *= convert_to_epsilon('snp', nuc, quality) * convert_to_cond_seq_error('snp', nuc, ground, quality)\n",
    "                count[PROB_MAPPING.get(nuc)] += 1\n",
    "                \n",
    "            if df['is_single'].loc[i] is True:\n",
    "                category[CATEGORY_MAPPING.get('is_single')] = 1\n",
    "                \n",
    "            # Update sample, label\n",
    "            sample[0:5] = count\n",
    "            sample[5:10] = prob\n",
    "            sample[10:14] = category\n",
    "            \n",
    "            label[PROB_MAPPING.get(truth)] = 1\n",
    "            \n",
    "            samples.append(sample)\n",
    "            labels.append(label)\n",
    "            \n",
    "        return samples, labels\n",
    "    \n",
    "    def single_df_run(self, df_idx):\n",
    "        \n",
    "        df = self.dfs[df_idx]\n",
    "        if df.empty: return\n",
    "        \n",
    "        cov = get_cov(df)\n",
    "        if self.enforce_cov:\n",
    "            if cov < self.cov_min or cov > self.cov_max: return\n",
    "            \n",
    "            \n",
    "        reads_idxs = [i for i in range(1, cov+1)]\n",
    "        if cov > self.reads_max:\n",
    "            reads_to_use = np.random.sample(reads_idxs)[0:self.reads_max]\n",
    "        else:\n",
    "            reads_to_use = reads_idxs\n",
    "            \n",
    "            \n",
    "        \n",
    "        # SNPS\n",
    "        snps = df.iloc[np.where( (df['is_snp'] == True))[0]]\n",
    "        \n",
    "        # Indels\n",
    "        ins = df.iloc[np.where( (df['is_ins'] == True))[0]]\n",
    "        dels = pd_filter(df, ['is_indel', 'is_ins'], [True, False])\n",
    "        \n",
    "        # Regular reads\n",
    "        regs = pd_filter(df, ['is_indel', 'is_snp'], [False, False])\n",
    "        \n",
    "        def extend_data(self, s, l):\n",
    "            self.all_samples.extend(s)\n",
    "            self.all_labels.extend(l)\n",
    "\n",
    "        if self.composition:\n",
    "        # Actually update it\n",
    "            if self.count_snp < self.max_snp:\n",
    "                if not snps.empty:\n",
    "                    s, l = sample_label_generator(df, snps, cov, reads_to_use, is_snps=True)\n",
    "                    self.count_snp += len(l)\n",
    "                    extend_data(s, l)\n",
    "                \n",
    "            if self.count_ins < self.max_ins:\n",
    "                if not ins.empty:\n",
    "                    s, l = sample_label_generator(df, ins, cov, reads_to_use, is_ins=True)\n",
    "                    self.count_ins += len(l)\n",
    "                    extend_data(s, l)\n",
    "            \n",
    "            if self.count_del < self.max_del:\n",
    "                if not dels.empty:\n",
    "                    s, l = sample_label_generator(df, dels, cov, reads_to_use)\n",
    "                    self.count_del += len(l)\n",
    "                    extend_data(s, l)\n",
    "            \n",
    "            if self.count_reg < self.max_reg:\n",
    "                if not regs.empty:\n",
    "                    s, l = sample_label_generator(df, regs, cov, reads_to_use, is_snps=True)\n",
    "                    self.count_reg += len(l)\n",
    "                    extend_data(s, l)\n",
    "        else:\n",
    "            if not snps.empty:\n",
    "                s, l = sample_label_generator(df, snps, cov, reads_to_use, is_snps=True)\n",
    "                extend_data(s, l)\n",
    "            \n",
    "            if not ins.empty:\n",
    "                s, l = sample_label_generator(df, ins, cov, reads_to_use, is_ins=True)\n",
    "                extend_data(s, l)\n",
    "        \n",
    "            if not dels.empty:\n",
    "                s, l = sample_label_generator(df, dels, cov, reads_to_use)\n",
    "                extend_data(s, l)\n",
    "        \n",
    "            if not regs.empty:\n",
    "                s, l = sample_label_generator(df, regs, cov, reads_to_use, is_snps=True)\n",
    "                extend_data(s, l)\n",
    "                \n",
    "    def dflist_run(self, random_select=False):\n",
    "        if random_select:\n",
    "            random_idxs = np.random.permutation(self.total_assembly_matrices)\n",
    "        \n",
    "        for n, idx in enumerate(random_idxs):\n",
    "            get_progress(n, each=True)\n",
    "            \n",
    "            single_df_run(self, idx)\n",
    "        return self.all_samples, self.all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sample_label_generator() got multiple values for argument 'is_snps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/neurotoolbox/Desktop/David/mutalib/features.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=0'>1</a>\u001b[0m samples, labels \u001b[39m=\u001b[39m get_data(BFP_LIST, random_select\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/neurotoolbox/Desktop/David/mutalib/features.ipynb Cell 30\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(df_list, rfp, enforce_cov, cov_min, cov_max, reads_max, total, multiple_sizes, multiple_sampling, random_select)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=131'>132</a>\u001b[0m         df \u001b[39m=\u001b[39m df_list[idx]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=132'>133</a>\u001b[0m         \u001b[39mif\u001b[39;00m df\u001b[39m.\u001b[39mempty: \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=134'>135</a>\u001b[0m         single_df_run(df, all_samples, all_labels, reads_max)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=135'>136</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=136'>137</a>\u001b[0m     \u001b[39mfor\u001b[39;00m n, df \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(df_list):\n",
      "\u001b[1;32m/home/neurotoolbox/Desktop/David/mutalib/features.ipynb Cell 30\u001b[0m in \u001b[0;36msingle_df_run\u001b[0;34m(df, all_samples, all_labels, reads_max, rfp, enforce_cov, cov_min, cov_max)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=45'>46</a>\u001b[0m \u001b[39m# if count_reg < max_reg:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m regs\u001b[39m.\u001b[39mempty:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=47'>48</a>\u001b[0m     s, l \u001b[39m=\u001b[39m sample_label_generator(df, regs, cov, reads_to_use, all_samples, all_labels, is_snps\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=48'>49</a>\u001b[0m     count_reg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(l)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/neurotoolbox/Desktop/David/mutalib/features.ipynb#ch0000021?line=49'>50</a>\u001b[0m     extend_data(s, l)\n",
      "\u001b[0;31mTypeError\u001b[0m: sample_label_generator() got multiple values for argument 'is_snps'"
     ]
    }
   ],
   "source": [
    "RedTrainingSet = DFDataSet(RFP_LIST, CEM_INDELS, CEM_SNIPS, QADJ_INDELS, QADJ_SNIPS, is_rfp=True, enforce_cov=True, cov_min=4, cov_max=9, reads_max=3)\n",
    "RedTestSet = DFDataSet(RFP_LIST, CEM_INDELS, CEM_SNIPS, QADJ_INDELS, QADJ_SNIPS, is_rfp=True, enforce_cov=True, cov_min=10, cov_max=20, read_max=3)\n",
    "\n",
    "BlueDataSet = DFDataSet(BFP_LIST, CEM_INDELS, CEM_SNIPS, QADJ_INDELS, QADJ_SNIPS, reads_max=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rx, train_ry = get_data(RFP_LIST, rfp=True, enforce_cov=True, cov_min=4, cov_max=9)\n",
    "train_bx, train_by = get_data(BFP_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rx, test_ry = get_data(RFP_LIST, rfp=True, enforce_cov=True, cov_min=10)\n",
    "test_bx, test_by = get_data(BFP_LIST, reads_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e45b5fe9540dcae422003c6a552f47067b28c6171ef36dc1ede64a9e99e7c78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
